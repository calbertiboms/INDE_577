{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "This notebook explores logistic regression to predict shipment delays using a supply chain logistics dataset.\n",
    "\n",
    "Objectives:\n",
    "Preprocess the dataset for machine learning.\n",
    "Implement logistic regression using sklearn and a custom implementation.\n",
    "Evaluate the models' performance.\n",
    "Compare different approaches for feature selection and model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data loading\n",
    "We will load the dataset and perform a preliminary check for missing values or inconsistencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order ID</th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Origin Port</th>\n",
       "      <th>Carrier</th>\n",
       "      <th>TPT</th>\n",
       "      <th>Service Level</th>\n",
       "      <th>Ship ahead day count</th>\n",
       "      <th>Ship Late Day count</th>\n",
       "      <th>Customer</th>\n",
       "      <th>Product ID</th>\n",
       "      <th>Plant Code</th>\n",
       "      <th>Destination Port</th>\n",
       "      <th>Unit quantity</th>\n",
       "      <th>Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.447296e+09</td>\n",
       "      <td>2013-05-26</td>\n",
       "      <td>PORT09</td>\n",
       "      <td>V44_3</td>\n",
       "      <td>1</td>\n",
       "      <td>CRF</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>V55555_53</td>\n",
       "      <td>1700106</td>\n",
       "      <td>PLANT16</td>\n",
       "      <td>PORT09</td>\n",
       "      <td>808</td>\n",
       "      <td>14.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.447158e+09</td>\n",
       "      <td>2013-05-26</td>\n",
       "      <td>PORT09</td>\n",
       "      <td>V44_3</td>\n",
       "      <td>1</td>\n",
       "      <td>CRF</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>V55555_53</td>\n",
       "      <td>1700106</td>\n",
       "      <td>PLANT16</td>\n",
       "      <td>PORT09</td>\n",
       "      <td>3188</td>\n",
       "      <td>87.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.447139e+09</td>\n",
       "      <td>2013-05-26</td>\n",
       "      <td>PORT09</td>\n",
       "      <td>V44_3</td>\n",
       "      <td>1</td>\n",
       "      <td>CRF</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>V55555_53</td>\n",
       "      <td>1700106</td>\n",
       "      <td>PLANT16</td>\n",
       "      <td>PORT09</td>\n",
       "      <td>2331</td>\n",
       "      <td>61.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.447364e+09</td>\n",
       "      <td>2013-05-26</td>\n",
       "      <td>PORT09</td>\n",
       "      <td>V44_3</td>\n",
       "      <td>1</td>\n",
       "      <td>CRF</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>V55555_53</td>\n",
       "      <td>1700106</td>\n",
       "      <td>PLANT16</td>\n",
       "      <td>PORT09</td>\n",
       "      <td>847</td>\n",
       "      <td>16.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.447364e+09</td>\n",
       "      <td>2013-05-26</td>\n",
       "      <td>PORT09</td>\n",
       "      <td>V44_3</td>\n",
       "      <td>1</td>\n",
       "      <td>CRF</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>V55555_53</td>\n",
       "      <td>1700106</td>\n",
       "      <td>PLANT16</td>\n",
       "      <td>PORT09</td>\n",
       "      <td>2163</td>\n",
       "      <td>52.34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Order ID Order Date Origin Port Carrier  TPT Service Level  \\\n",
       "0  1.447296e+09 2013-05-26      PORT09   V44_3    1           CRF   \n",
       "1  1.447158e+09 2013-05-26      PORT09   V44_3    1           CRF   \n",
       "2  1.447139e+09 2013-05-26      PORT09   V44_3    1           CRF   \n",
       "3  1.447364e+09 2013-05-26      PORT09   V44_3    1           CRF   \n",
       "4  1.447364e+09 2013-05-26      PORT09   V44_3    1           CRF   \n",
       "\n",
       "   Ship ahead day count  Ship Late Day count   Customer  Product ID  \\\n",
       "0                     3                    0  V55555_53     1700106   \n",
       "1                     3                    0  V55555_53     1700106   \n",
       "2                     3                    0  V55555_53     1700106   \n",
       "3                     3                    0  V55555_53     1700106   \n",
       "4                     3                    0  V55555_53     1700106   \n",
       "\n",
       "  Plant Code Destination Port  Unit quantity  Weight  \n",
       "0    PLANT16           PORT09            808   14.30  \n",
       "1    PLANT16           PORT09           3188   87.94  \n",
       "2    PLANT16           PORT09           2331   61.20  \n",
       "3    PLANT16           PORT09            847   16.16  \n",
       "4    PLANT16           PORT09           2163   52.34  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "file_path = '/Users/ceciliaalberti/Documents/INDE_577/datasets/Supply chain logisitcs problem.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data preprocessing\n",
    "This section handles:\n",
    "\n",
    "1. Creating the target variable.\n",
    "2. Selecting features and target.\n",
    "3. Scaling numerical features\n",
    "4. Encoding categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of           Order ID Order Date Origin Port Carrier  TPT Service Level  \\\n",
       "0     1.447296e+09 2013-05-26      PORT09   V44_3    1           CRF   \n",
       "1     1.447158e+09 2013-05-26      PORT09   V44_3    1           CRF   \n",
       "2     1.447139e+09 2013-05-26      PORT09   V44_3    1           CRF   \n",
       "3     1.447364e+09 2013-05-26      PORT09   V44_3    1           CRF   \n",
       "4     1.447364e+09 2013-05-26      PORT09   V44_3    1           CRF   \n",
       "...            ...        ...         ...     ...  ...           ...   \n",
       "9210  1.447305e+09 2013-05-26      PORT04  V444_1    1           DTD   \n",
       "9211  1.447319e+09 2013-05-26      PORT04  V444_1    1           DTD   \n",
       "9212  1.447322e+09 2013-05-26      PORT04  V444_1    1           DTD   \n",
       "9213  1.447145e+09 2013-05-26      PORT04  V444_1    1           DTD   \n",
       "9214  1.447328e+09 2013-05-26      PORT04  V444_1    1           DTD   \n",
       "\n",
       "      Ship ahead day count  Ship Late Day count           Customer  \\\n",
       "0                        3                    0          V55555_53   \n",
       "1                        3                    0          V55555_53   \n",
       "2                        3                    0          V55555_53   \n",
       "3                        3                    0          V55555_53   \n",
       "4                        3                    0          V55555_53   \n",
       "...                    ...                  ...                ...   \n",
       "9210                     5                    0  V55555555555555_8   \n",
       "9211                     5                    0  V55555555555555_8   \n",
       "9212                     5                    0  V55555555555555_8   \n",
       "9213                     5                    0  V55555555555555_8   \n",
       "9214                     5                    0  V55555555555555_8   \n",
       "\n",
       "      Product ID Plant Code Destination Port  Unit quantity     Weight  \n",
       "0        1700106    PLANT16           PORT09            808  14.300000  \n",
       "1        1700106    PLANT16           PORT09           3188  87.940000  \n",
       "2        1700106    PLANT16           PORT09           2331  61.200000  \n",
       "3        1700106    PLANT16           PORT09            847  16.160000  \n",
       "4        1700106    PLANT16           PORT09           2163  52.340000  \n",
       "...          ...        ...              ...            ...        ...  \n",
       "9210     1683388    PLANT03           PORT09            339   2.354118  \n",
       "9211     1683388    PLANT03           PORT09            339   2.354118  \n",
       "9212     1683388    PLANT03           PORT09            245   0.294265  \n",
       "9213     1683430    PLANT03           PORT09            278   2.480000  \n",
       "9214     1683424    PLANT03           PORT09            317   1.960000  \n",
       "\n",
       "[9215 rows x 14 columns]>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the target column: 1 for delayed shipments, 0 for on-time shipments\n",
    "df['Delayed'] = df['Ship Late Day count'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Select features and target\n",
    "features = ['Origin Port', 'Carrier', 'Ship ahead day count', 'Origin Port', 'Plant Code', 'Destination Port', 'Weight']\n",
    "X = df[features]\n",
    "y = df['Delayed']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_scaler(X, cols_to_scale):\n",
    "    X_scaled = X.copy()\n",
    "    for col in cols_to_scale:\n",
    "        mean = X[col].mean()  # Calculate mean\n",
    "        std = X[col].std()    # Calculate standard deviation\n",
    "        X_scaled[col] = (X[col] - mean) / std  \n",
    "    return X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical features to scale\n",
    "numerical_features = ['Ship ahead day count', 'Weight']\n",
    "\n",
    "# Apply manual scaling to numerical features\n",
    "X_scaled = standard_scaler(X, numerical_features)\n",
    "\n",
    "# Encode categorical features (leave scaled numerical features unchanged)\n",
    "X_encoded = pd.get_dummies(X_scaled, columns=['Origin Port', 'Carrier', 'Plant Code', 'Destination Port'], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Custom Logistic Regression Implementation\n",
    "We implement logistic regression from scratch using gradient descent for better understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescentLogisticReg:\n",
    "    def __init__(self) -> None:\n",
    "        self.X = None\n",
    "        self.variables = None\n",
    "        self.y = None\n",
    "        self.predictor = None\n",
    "        self.n = None\n",
    "        self.p = None\n",
    "        self.bias = None\n",
    "        self.gamma = None\n",
    "        self.max_iter = None\n",
    "        self.eta = None\n",
    "        self.weights = None\n",
    "        self.weights_history = []\n",
    "        self.loss_history = [np.inf]\n",
    "\n",
    "    # Cross entropy loss for one data point\n",
    "    def cross_entropy_loss(self, y, y_hat):\n",
    "        epsilon = 1e-10  # Small value to avoid log(0)\n",
    "        y_hat = np.clip(y_hat, epsilon, 1 - epsilon)  # Clip y_hat to avoid extreme values\n",
    "        return -y * np.log(y_hat) - (1.0 - y) * np.log(1.0 - y_hat)\n",
    "\n",
    "    # Total cross entropy loss\n",
    "    def loss(self):\n",
    "        total_loss = sum(self.cross_entropy_loss(self.y[i], self.sigmoid(x @ self.weights))\n",
    "                         for i, x in enumerate(self.X))\n",
    "        return total_loss\n",
    "\n",
    "    # Sigmoid function\n",
    "    def sigmoid(self, z):\n",
    "        # Clip the input values to avoid extremely large values that cause overflow\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "    # Gradient of the loss function\n",
    "    def gradient_L(self):\n",
    "        sigmoids = np.array([self.sigmoid(x @ self.weights) - self.y[i] for i, x in enumerate(self.X)])\n",
    "        d_w = sigmoids @ self.X\n",
    "        return d_w\n",
    "\n",
    "    # Model fitting with gradient descent\n",
    "    def fit(self, X, y, bias=True, gamma=0.01, max_iter=1000, eta=0.001, patience=10):\n",
    "        self.variables = X.columns\n",
    "        self.predictor = y.name\n",
    "\n",
    "        X = X.to_numpy()\n",
    "        y = y.to_numpy()\n",
    "        if bias:\n",
    "            ones_column = np.ones((X.shape[0], 1))\n",
    "            X = np.append(ones_column, X, axis=1)\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.n = X.shape[0]\n",
    "        self.p = X.shape[1]\n",
    "        self.bias = bias\n",
    "        self.gamma = gamma\n",
    "        self.max_iter = max_iter\n",
    "        self.eta = eta\n",
    "\n",
    "        weights = np.random.rand(self.p) * 0.01  # Initialize weights small but non-zero\n",
    "        self.weights = weights\n",
    "        self.weights_history.append(weights)\n",
    "\n",
    "        patience_counter = 0\n",
    "\n",
    "        for i in range(1, max_iter + 1):\n",
    "            dw = self.gradient_L()\n",
    "            weights = weights - gamma * dw\n",
    "            self.weights = weights\n",
    "            self.weights_history.append(weights)\n",
    "            L = self.loss()\n",
    "            self.loss_history.append(L)\n",
    "\n",
    "            # Print weights every 100 iterations for debugging\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Iteration {i}: Loss = {L}, Weights = {self.weights[:5]}...\")\n",
    "\n",
    "            # Adaptive learning rate (reduce gamma if loss does not decrease sufficiently)\n",
    "            if i > 1 and L > self.loss_history[i - 1]:\n",
    "                gamma *= 0.9  # Reduce learning rate by 10% if loss increases\n",
    "\n",
    "            # Early stopping with patience\n",
    "            if i > 1 and abs(L - self.loss_history[i - 1]) <= self.eta:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"Stopping early at iteration {i}\")\n",
    "                    break\n",
    "            else:\n",
    "                patience_counter = 0\n",
    "\n",
    "    # Predict new data\n",
    "    def prediction(self, X, weights=None):\n",
    "        if weights is None:\n",
    "            weights = self.weights\n",
    "\n",
    "        X = X.to_numpy()\n",
    "        if self.bias:\n",
    "            ones_column = np.ones((X.shape[0], 1))\n",
    "            X = np.append(ones_column, X, axis=1)\n",
    "\n",
    "        labels = np.array([1, 0])\n",
    "        y_hat = [self.sigmoid(x @ weights) for x in X]\n",
    "        return [np.random.choice(labels, p=[y_hat_i, 1.0 - y_hat_i]) for y_hat_i in y_hat]\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    def accuracy(self, y_true, y_pred):\n",
    "        correct_predictions = sum(y_true == y_pred)  # Count correct predictions\n",
    "        total_predictions = len(y_true)             # Total number of predictions\n",
    "        accuracy = correct_predictions / total_predictions\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample the majority class (shipments NOT delayed, target 0) to 1000 data points\n",
    "majority_class = X_encoded[y == 0]\n",
    "minority_class = X_encoded[y == 1]\n",
    "y_majority = y[y == 0]\n",
    "y_minority = y[y == 1]\n",
    "\n",
    "majority_downsampled = majority_class.sample(n=1000, random_state=42)\n",
    "y_majority_downsampled = y_majority.sample(n=1000, random_state=42)\n",
    "\n",
    "X_downsampled = pd.concat([majority_downsampled, minority_class]).reset_index(drop=True)\n",
    "y_downsampled = pd.concat([y_majority_downsampled, y_minority]).reset_index(drop=True)\n",
    "\n",
    "# Shuffle the downsampled dataset\n",
    "X_downsampled, y_downsampled = shuffle(X_downsampled, y_downsampled, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_encoded, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsampling + PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=0.9)  # Retain 90% of the variance\n",
    "X_pca_downsampling = pca.fit_transform(X_downsampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE + PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=0.9)  # Retain 90% of the variance\n",
    "X_pca_smote = pca.fit_transform(X_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training and Evaluation\n",
    "We train the model and evaluate its performance using accuracy, classification report and confusion matrix. We will perform the model under different circumstances:\n",
    "1. With downsampled data for the majority class\n",
    "2. With SMOTE\n",
    "3. With downsampling + PCA\n",
    "4. With SMOTE + PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsampling results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100: Loss = 361.71505218227213, Weights = [-1.482868136542647 0.2894185334800641 -0.6173423102783446\n",
      " 0.006577461377543368 -0.49336795593002275]...\n",
      "Iteration 200: Loss = 360.4479545762468, Weights = [-1.466953645522126 0.2943436006237874 -0.5944489972744605\n",
      " 0.006577461377543368 -0.6524717677283971]...\n",
      "Iteration 300: Loss = 359.94090709866805, Weights = [-1.4607810631192233 0.29655897520658825 -0.5858964817120832\n",
      " 0.006577461377543368 -0.7588526470604504]...\n",
      "Iteration 400: Loss = 359.6637256161409, Weights = [-1.457454725848399 0.2978749095118569 -0.5814412248910132\n",
      " 0.006577461377543368 -0.8392098790198672]...\n",
      "Iteration 500: Loss = 359.4878548718665, Weights = [-1.4553624389522384 0.29876132237167313 -0.5787083777096306\n",
      " 0.006577461377543368 -0.9038733940231245]...\n",
      "Iteration 600: Loss = 359.36595072313924, Weights = [-1.453920786080321 0.2994041190377696 -0.5768623704886447\n",
      " 0.006577461377543368 -0.958003009897652]...\n",
      "Stopping early at iteration 621\n",
      "Accuracy of Gradient Descent Logistic Regression Model: 0.6861924686192469\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.81      0.81       197\n",
      "           1       0.10      0.10      0.10        42\n",
      "\n",
      "    accuracy                           0.69       239\n",
      "   macro avg       0.45      0.45      0.45       239\n",
      "weighted avg       0.68      0.69      0.68       239\n",
      "\n",
      "Confusion Matrix:\n",
      "[[160  37]\n",
      " [ 38   4]]\n"
     ]
    }
   ],
   "source": [
    "# Split the resampled dataset into training and testing sets\n",
    "# The dataset is split into training and testing sets to evaluate the model.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_downsampled, y_downsampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the custom logistic regression model\n",
    "# The model is fitted using the training data. Gradient descent is used to optimize the model parameters.\n",
    "gd_log_reg = GradientDescentLogisticReg()\n",
    "gd_log_reg.fit(pd.DataFrame(X_train), pd.Series(y_train), gamma=0.01, max_iter=1000, eta=0.001)\n",
    "\n",
    "# Predict the test set\n",
    "# Predictions are made for the test set using the fitted model.\n",
    "y_pred = gd_log_reg.prediction(pd.DataFrame(X_test))\n",
    "\n",
    "# Calculate accuracy\n",
    "# The accuracy of the model on the test set is calculated.\n",
    "accuracy = gd_log_reg.accuracy(y_test, y_pred)\n",
    "print(\"Accuracy of Gradient Descent Logistic Regression Model:\", accuracy)\n",
    "\n",
    "# Classification report and confusion matrix\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Classification report and confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is high because class 0 (on-time shipments) dominates, and the model performs well for class 0. However, the recall for class 1 (delayed shipments) is low, meaning that the model struggles to detect delayed shipments. The problem is that the model is biased towards predicting on-time shipments, which indicates that there is stil a class imbalance that is causing the model to perform poorly on the minority class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100: Loss = 8570.805724238204, Weights = [0.196769587667199 0.14219297241950424 -0.3038963749997996\n",
      " -0.044994346640886314 -3.5815843940283276]...\n",
      "Iteration 200: Loss = 8448.484162189501, Weights = [0.2500873433002569 0.15069184315866366 -0.3156036298568801\n",
      " -0.04499434664097596 -3.581636214363067]...\n",
      "Iteration 300: Loss = 8446.033886297988, Weights = [0.2569819502898078 0.15451046948299735 -0.3175859315794989\n",
      " -0.044994346641067474 -3.5816890350838713]...\n",
      "Iteration 400: Loss = 8445.683547287521, Weights = [0.2575043974622756 0.15476302126562988 -0.31765835591183567\n",
      " -0.04499434664115907 -3.5817419749270716]...\n",
      "Iteration 500: Loss = 8445.398787017675, Weights = [0.25766402878442535 0.15489243438075556 -0.3176523931453922\n",
      " -0.04499434664125066 -3.5817949188004965]...\n",
      "Iteration 600: Loss = 8445.163211870817, Weights = [0.2577606843383 0.15500273303137732 -0.31764023471473585\n",
      " -0.044994346641342255 -3.5818478622245014]...\n",
      "Iteration 700: Loss = 8444.965951887501, Weights = [0.25783463090053654 0.1550980576248492 -0.3176280805590158\n",
      " -0.04499434664143385 -3.5819008039964046]...\n",
      "Iteration 800: Loss = 8444.798918148477, Weights = [0.2578964249826291 0.15518072572330657 -0.31761698057384646\n",
      " -0.04499434664152544 -3.581953743189921]...\n",
      "Iteration 900: Loss = 8444.656020386985, Weights = [0.2579497735127818 0.15525280911862502 -0.31760701201136404\n",
      " -0.044994346641617035 -3.5820066790423803]...\n",
      "Iteration 1000: Loss = 8444.532619437075, Weights = [0.2579964880721564 0.1553160415517508 -0.3175980526304323\n",
      " -0.04499434664170863 -3.5820596109208296]...\n",
      "Accuracy of Gradient Descent Logistic Regression Model: 0.5914127423822715\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.60      0.60      1805\n",
      "           1       0.59      0.58      0.59      1805\n",
      "\n",
      "    accuracy                           0.59      3610\n",
      "   macro avg       0.59      0.59      0.59      3610\n",
      "weighted avg       0.59      0.59      0.59      3610\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1086  719]\n",
      " [ 756 1049]]\n"
     ]
    }
   ],
   "source": [
    "# Split the resampled dataset into training and testing sets\n",
    "# The dataset is split into training and testing sets to evaluate the model.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the custom logistic regression model\n",
    "# The model is fitted using the training data. Gradient descent is used to optimize the model parameters.\n",
    "gd_log_reg = GradientDescentLogisticReg()\n",
    "gd_log_reg.fit(pd.DataFrame(X_train), pd.Series(y_train), gamma=0.01, max_iter=1000, eta=0.001)\n",
    "\n",
    "# Predict the test set\n",
    "# Predictions are made for the test set using the fitted model.\n",
    "y_pred = gd_log_reg.prediction(pd.DataFrame(X_test))\n",
    "\n",
    "# Calculate accuracy\n",
    "# The accuracy of the model on the test set is calculated.\n",
    "accuracy = gd_log_reg.accuracy(y_test, y_pred)\n",
    "print(\"Accuracy of Gradient Descent Logistic Regression Model:\", accuracy)\n",
    "\n",
    "# Classification report and confusion matrix\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Classification report and confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After SMOTE, the dataset is balanced, which theoretically helps with classification. However, it also introduces synthetic examples that might not improve the model’s ability to generalize if the feature relationships are not well captured. This is evident from the balanced but poor performance (precision, recall, F1-score all around 0.57)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsampling + PCA results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping early at iteration 58\n",
      "Accuracy of Gradient Descent Logistic Regression Model: 0.7405857740585774\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.85      0.84       197\n",
      "           1       0.24      0.21      0.23        42\n",
      "\n",
      "    accuracy                           0.74       239\n",
      "   macro avg       0.54      0.53      0.53       239\n",
      "weighted avg       0.73      0.74      0.74       239\n",
      "\n",
      "Confusion Matrix:\n",
      "[[168  29]\n",
      " [ 33   9]]\n"
     ]
    }
   ],
   "source": [
    "# Split the resampled dataset into training and testing sets\n",
    "# The dataset is split into training and testing sets to evaluate the model.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca_downsampling, y_downsampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the custom logistic regression model\n",
    "# The model is fitted using the training data. Gradient descent is used to optimize the model parameters.\n",
    "gd_log_reg = GradientDescentLogisticReg()\n",
    "gd_log_reg.fit(pd.DataFrame(X_train), pd.Series(y_train), gamma=0.01, max_iter=1000, eta=0.001)\n",
    "\n",
    "# Predict the test set\n",
    "# Predictions are made for the test set using the fitted model.\n",
    "y_pred = gd_log_reg.prediction(pd.DataFrame(X_test))\n",
    "\n",
    "# Calculate accuracy\n",
    "# The accuracy of the model on the test set is calculated.\n",
    "accuracy = gd_log_reg.accuracy(y_test, y_pred)\n",
    "print(\"Accuracy of Gradient Descent Logistic Regression Model:\", accuracy)\n",
    "\n",
    "# Classification report and confusion matrix\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Classification report and confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE + PCA results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100: Loss = 9420.026323475224, Weights = [-0.0083652   0.05863385 -0.61133997 -1.50728728]...\n",
      "Stopping early at iteration 164\n",
      "Accuracy of Gradient Descent Logistic Regression Model: 0.5487534626038781\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.55      0.55      1805\n",
      "           1       0.55      0.55      0.55      1805\n",
      "\n",
      "    accuracy                           0.55      3610\n",
      "   macro avg       0.55      0.55      0.55      3610\n",
      "weighted avg       0.55      0.55      0.55      3610\n",
      "\n",
      "Confusion Matrix:\n",
      "[[989 816]\n",
      " [813 992]]\n"
     ]
    }
   ],
   "source": [
    "# Split the resampled dataset into training and testing sets\n",
    "# The dataset is split into training and testing sets to evaluate the model.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca_smote, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the custom logistic regression model\n",
    "# The model is fitted using the training data. Gradient descent is used to optimize the model parameters.\n",
    "gd_log_reg = GradientDescentLogisticReg()\n",
    "gd_log_reg.fit(pd.DataFrame(X_train), pd.Series(y_train), gamma=0.01, max_iter=1000, eta=0.001)\n",
    "\n",
    "# Predict the test set\n",
    "# Predictions are made for the test set using the fitted model.\n",
    "y_pred = gd_log_reg.prediction(pd.DataFrame(X_test))\n",
    "\n",
    "# Calculate accuracy\n",
    "# The accuracy of the model on the test set is calculated.\n",
    "accuracy = gd_log_reg.accuracy(y_test, y_pred)\n",
    "print(\"Accuracy of Gradient Descent Logistic Regression Model:\", accuracy)\n",
    "\n",
    "# Classification report and confusion matrix\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Classification report and confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PCA doesn't improve the results from the SMOTE. Again, the dataset is balanced, but the SMOTE introduced synthetic examples that didn't improve the model as the feature relationships are not well captured. Balanced but poor performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best model\n",
    "The downsampling + PCA model is the best-performing model based on these results. It achieves a good balance of overall accuracy (74.9%) and shows an improvement in recall (0.37) for the minority class compared to the other models. The f1-score and precision for class 1 are also higher than the other models, indicating that it makes more successful predictions for delayed shipments while maintaining a reasonable overall accuracy. This suggests that PCA helps retain key information while reducing dimensionality, improving the model's ability to recognize delayed shipments in a more balanced way compared to the purely downsampled or SMOTE models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
